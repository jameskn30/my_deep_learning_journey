{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNn5gHjqfE1V1ygUMgUXv/V"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Coding BERT from scratch again from the d2l book"],"metadata":{"id":"866wVMFcyHCP"}},{"cell_type":"code","source":["!pip install setuptools==66\n","!pip install matplotlib_inline\n","!pip install d2l==1.0.0b\n","!pip install pytorch-pretrained-bert"],"metadata":{"id":"l0dIVoa84DtF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from d2l import torch as d2l"],"metadata":{"id":"zXms4gYZ4WNQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BERTEncoder(nn.Module):\n","  def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads, num_blocks = 2, dropout = 0.2, max_len = 1000):\n","    super().__init__()\n","    self.embedding = nn.Embedding(vocab_size, num_hiddens)\n","    self.seg_embedding = nn.Embedding(2, num_hiddens)\n","    self.pos_embedding = nn.Parameter(torch.rand(1, max_len, num_hiddens))\n","    self.blocks = nn.Sequential()\n","    for i in range(num_blocks):\n","      self.blocks.add_module(f'{i}',\n","          d2l.TransformerEncoderBlock(\n","              num_hiddens, ffn_num_hiddens, num_heads, dropout, True))\n","  \n","  #tokens shape (batch, max_len)\n","  def forward(self, tokens, segments, valid_lens):\n","    X = self.embedding(tokens) + self.seg_embedding(segments)\n","    #only take up to tokens length\n","    X = X + self.pos_embedding[:, :X.shape[1], :]\n","    for block in self.blocks:\n","      X = block(X, valid_lens)\n","    \n","    return X\n","\n","class MaskLM(nn.Module):\n","  def __init__(self, vocab_size, num_hiddens, **kwargs):\n","    super(MaskLM, self).__init__(**kwargs)\n","    self.mlp = nn.Sequential(\n","        nn.LazyLinear(num_hiddens),\n","        nn.ReLU(),\n","        nn.LayerNorm(num_hiddens),\n","        nn.LazyLinear(vocab_size),\n","    )\n","    \n","  def forward(self, X, pred_positions):\n","    #X shape (batch, max_len, # hiddens)\n","    #pred positions shape (# batch, # preds)\n","    num_preds = pred_positions.shape[1]\n","    batch_size = X.shape[0]\n","    pred_positions = pred_positions.reshape(-1)\n","    batch_idx = torch.arange(0, batch_size)\n","    batch_idx = torch.repeat_interleave(batch_idx, num_preds)\n","\n","    print(pred_positions.shape)\n","    print(batch_idx.shape)\n","\n","    masked_X = X[batch_idx,pred_positions]\n","    #reshape it\n","    masked_X = masked_X.reshape((batch_size, num_preds, -1))\n","    mlm_y_hat = self.mlp(masked_X)\n","    return mlm_y_hat\n","class NextSentencePred(nn.Module):\n","\n","  def __init__(self, **kwargs):\n","    super(NextSentencePred, self).__init__(**kwargs)\n","    self.output = nn.LazyLinear(2)\n","\n","  def forward(self, X):\n","    # X shape (# batch, num_hiddens)\n","    return self.output(X) \n","\n","class BERTModel(nn.Module):\n","  def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads, num_blocks = 2, dropout = 0.2, max_len = 100, **kwargs):\n","    super(BERTModel, self).__init__(**kwargs)\n","    self.encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens, num_heads, num_blocks, dropout, max_len)\n","    self.hidden = nn.Sequential(\n","        nn.LazyLinear(num_hiddens),\n","        nn.Tanh())\n","    self.mlm = MaskLM(vocab_size, num_hiddens)\n","    self.nsp = NextSentencePred()\n","  \n","  def forward(self, tokens, segments, pred_positions):\n","    encoded_X = self.encoder(tokens, segments, None)\n","    if pred_positions == None:\n","      mlm_Y_pred = None\n","    else:\n","      mlm_Y_pred = self.mlm(encoded_X, pred_positions)\n","    nsp_Y_pred = self.nsp(self.hidden(encoded_X[:,0,:]))\n","\n","    return encoded_X, mlm_Y_pred, nsp_Y_pred"],"metadata":{"id":"QZWYf5dbALea"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size, num_hiddens, ffn_num_hiddens, num_heads, num_blocks, dropout = 1000, 768, 1024, 4, 2, 0.2\n","model = BERTModel(vocab_size, num_hiddens, ffn_num_hiddens, num_heads, num_blocks, dropout)\n","\n","batch_size = 2 \n","token_len = 10\n","tokens = torch.randint(0, vocab_size, (batch_size, token_len))\n","print(tokens.shape)\n","segments = torch.tensor(([0,0,0,0,0,0,1,1,1,1], [0,0,0,0,0,1,1,1,1,1]))\n","print(segments.shape)\n","pred_positions = torch.tensor([[1,3,4,5], [3,6,7,1]])\n","print(pred_positions.shape)\n","\n","encoded_X, mlm_Y_pred, nsp_Y_pred = model(tokens, segments, pred_positions)\n","\n","print('encoded_X shape', encoded_X.shape)\n","print('mlm_Y_pred shape', mlm_Y_pred.shape)\n","print('nsp_Y_pred shape', nsp_Y_pred.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LXRrqlqeQu53","executionInfo":{"status":"ok","timestamp":1682693396872,"user_tz":240,"elapsed":803,"user":{"displayName":"James Nguyen (JK)","userId":"02481723335785605125"}},"outputId":"e9139997-b528-4b2f-c6e9-50b2882261e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 10])\n","torch.Size([2, 10])\n","torch.Size([2, 4])\n","torch.Size([8])\n","torch.Size([8])\n","encoded_X shape torch.Size([2, 10, 768])\n","mlm_Y_pred shape torch.Size([2, 4, 1000])\n","nsp_Y_pred shape torch.Size([2, 2])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"OZGYhP0XQ8Bf"},"execution_count":null,"outputs":[]}]}