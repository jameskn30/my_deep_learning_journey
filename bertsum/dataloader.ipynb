{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "https://deeplearninganalytics.org/text-summarization/#:~:text=Text%20Summarization%20using%20BERT%201%20Introduction%20Single-document%20text,Pulling%20the%20code%20and%20testing%20this%20out%20\n",
    "\n",
    "BERT sum github: https://github.com/nlpyang/BertSum\n",
    "\n",
    "Following BERTSum paper - Yang Liu 2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import re\n",
    "\n",
    "from utils.vocab import Vocab\n",
    "import torchtext\n",
    "#run this in terminal\n",
    "#NOTE: download spacy enc_core_web_sm for torch tokenizer to work\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = './dataset/bertsum_data/'\n",
    "contraction_path = './dataset/contractions.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ain't\": 'am not', \"aren't\": 'are not', \"can't\": 'cannot', \"can't've\": 'cannot have', \"'cause\": 'because', \"could've\": 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"didn't\": 'did not', \"doesn't\": 'does not', \"don't\": 'do not', \"hadn't\": 'had not', \"hadn't've\": 'had not have', \"hasn't\": 'has not', \"haven't\": 'have not', \"he'd\": 'he would', \"he'd've\": 'he would have', \"he'll\": 'he will', \"he's\": 'he is', \"how'd\": 'how did', \"how'll\": 'how will', \"how's\": 'how is', \"i'd\": 'i would', \"i'll\": 'i will', \"i'm\": 'i am', \"i've\": 'i have', \"isn't\": 'is not', \"it'd\": 'it would', \"it'll\": 'it will', \"it's\": 'it is', \"let's\": 'let us', \"ma'am\": 'madam', \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't\": 'might not', \"must've\": 'must have', \"mustn't\": 'must not', \"needn't\": 'need not', \"oughtn't\": 'ought not', \"shan't\": 'shall not', \"sha'n't\": 'shall not', \"she'd\": 'she would', \"she'll\": 'she will', \"she's\": 'she is', \"should've\": 'should have', \"shouldn't\": 'should not', \"that'd\": 'that would', \"that's\": 'that is', \"there'd\": 'there had', \"there's\": 'there is', \"they'd\": 'they would', \"they'll\": 'they will', \"they're\": 'they are', \"they've\": 'they have', \"wasn't\": 'was not', \"we'd\": 'we would', \"we'll\": 'we will', \"we're\": 'we are', \"we've\": 'we have', \"weren't\": 'were not', \"what'll\": 'what will', \"what're\": 'what are', \"what's\": 'what is', \"what've\": 'what have', \"where'd\": 'where did', \"where's\": 'where is', \"who'll\": 'who will', \"who's\": 'who is', \"won't\": 'will not', \"wouldn't\": 'would not', \"you'd\": 'you would', \"you'll\": 'you will', \"you're\": 'you are'}\n"
     ]
    }
   ],
   "source": [
    "#Read contraction json\n",
    "with open(contraction_path, 'r') as file:\n",
    "    content = file.read()\n",
    "    contractions = json.loads(content)\n",
    "print(contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "src, len=512,[101, 2002, 1005, 1055, 2006, 1996, 2648, 2559, 1999, 1010, 1998, 1996, 3193, 1997, 13489, 13619, 6968, 4492, 17994, 3849, 2000, 2562, 2893, 11737, 5017, 1012, 102, 101, 1037, 2739, 3034, 5958, 2013, 2010, 2047, 7728, 1999, 8252, 3607, 2104, 9363, 5596, 2074, 2129, 11737, 2010, 16746, 3711, 2000, 2022, 1010, 2004, 1996, 28368, 5969, 2343, 10865, 2008, 2010, 3677, 1011, 1011, 1998, 4022, 27398, 1011, 1011, 2001, 2025, 2105, 1998, 2018, 2589, 2210, 2000, 2191, 2010, 2994, 2062, 6625, 1012, 102, 101, 1000, 1045, 5136, 2008, 3607, 2442, 1998, 2038, 2000, 2552, 1010, 1000, 13619, 6968, 4492, 17994, 2409, 1037, 6887, 7911, 26807, 1997, 12060, 2040, 2018, 9240, 1999, 1996, 2103, 1997, 20996, 29473, 1011, 2006, 1011, 2123, 1010, 2379, 1996, 8772, 3675, 2007, 5924, 1998, 2055, 6352, 2661, 2148, 1997, 4924, 1012, 102, 101, 2002, 2106, 2025, 20648, 2054, 4506, 2002, 2001, 5327, 2005, 2021, 2081, 3154, 3183, 2002, 2001, 2559, 2000, 1012, 102, 101, 1000, 4209, 1996, 2839, 1997, 2720, 1012, 8748, 22072, 1010, 1045, 2572, 4527, 2339, 1010, 2039, 1005, 18681, 2085, 1010, 2002, 2038, 2815, 19868, 1010, 4333, 1012, 102, 101, 2008, 2003, 1996, 3160, 1012, 1000, 102, 101, 2295, 2002, 1998, 1996, 2845, 2343, 2018, 5720, 2058, 1996, 3042, 1010, 2027, 2018, 2025, 2777, 1010, 2056, 13619, 6968, 4492, 17994, 1010, 2040, 2596, 2000, 9530, 22119, 2008, 2002, 2106, 2025, 2113, 2073, 22072, 4832, 2006, 2010, 15068, 6238, 1998, 2001, 5327, 2005, 15563, 1011, 1011, 7188, 1996, 2048, 2273, 2131, 2362, 1012, 102, 101, 1000, 2004, 2574, 2004, 2023, 3116, 3138, 2173, 1010, 1045, 2097, 3305, 2010, 7729, 1012, 102, 101, 1012, 1012, 1012, 2009, 2052, 2025, 2022, 6149, 2085, 2005, 2033, 2000, 3713, 2055, 2054, 3607, 2323, 2079, 2085, 1012, 102, 101, 1012, 1012, 1012, 3607, 2442, 2224, 2035, 2049, 12020, 1999, 2344, 2000, 9462, 2023, 8488, 1010, 2023, 7404, 2029, 2651, 2003, 2635, 2173, 1999, 5924, 1012, 102, 101, 2009, 2003, 3697, 2005, 2033, 2000, 2360, 2054, 2064, 2272, 2055, 1010, 2021, 1045, 2052, 2066, 2000, 2360, 2153, 2008, 1045, 2572, 4937, 27203, 2135, 2114, 2151, 11099, 2007, 1996, 11074, 11109, 1997, 5924, 2004, 1037, 2110, 1012, 1000, 102, 101, 1996, 3295, 1997, 1996, 2739, 3034, 2993, 2104, 9363, 5596, 2010, 12477, 1012, 102, 101, 1000, 1045, 1005, 1049, 4930, 2011, 1996, 2755, 2008, 2002, 1005, 1055, 1999, 20996, 29473, 1011, 2006, 1011, 2123, 1998, 2025, 1999, 4924, 1998, 2008, 2002, 2038, 2018, 1037, 3042, 2655, 2007, 2343, 22072, 1998, 2025, 2777, 1010, 1000, 2280, 1057, 1012, 1055, 1012, 6059, 2000, 3607, 2745, 11338, 7011, 5313, 2409, 13229, 1012, 102, 101, 1000, 1045, 2245, 2002, 5228, 20006, 2008, 2343, 22072, 2038, 2025, 2042, 2062, 13079, 2000, 6985, 2032, 1012, 1000, 102, 101, 2002, 6563, 13619, 6968, 4492, 17994, 1005, 1055, 7928, 2004, 1000, 5751, 2008, 2002, 2003, 2025, 1999, 2204, 3061, 2007, 2010, 2783, 3677, 1012, 1000, 102, 101, 2005, 2010, 2112, 1010, 22072, 6936, 8973, 1999, 5924, 1999, 1037, 7026, 2655, 2007, 2647, 4177, 1010, 6911, 2075, 1996, 5197, 1997, 9992, 2019, 9686, 25015, 3508, 1997, 4808, 1010, 2429, 2000, 1037, 4861, 102]\n",
      "labels, len=16,[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]\n",
      "segs, len=512,[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "clss, len=16,[0, 27, 82, 135, 156, 185, 193, 249, 266, 288, 316, 357, 372, 428, 449, 476]\n",
      "src_txt, len=36,[\"he 's on the outside looking in , and the view of viktor yanukovych seems to keep getting dimmer .\", 'a news conference friday from his new quarters in southeastern russia underscored just how dim his prospects appear to be , as the ousted ukrainian president complained that his host -- and potential benefactor -- was not around and had done little to make his stay more comfortable .', '\" i consider that russia must and has to act , \" yanukovych told a phalanx of reporters who had assembled in the city of rostov-on-don , near the southwestern border with ukraine and about 700 miles south of moscow .', 'he did not specify what actions he was hoping for but made clear whom he was looking to .', '\" knowing the character of mr. vladimir putin , i am surprised why , up \\'til now , he has remained restrained , silent .', 'that is the question . \"', 'though he and the russian president had talked over the phone , they had not met , said yanukovych , who appeared to concede that he did not know where putin stands on his ouster and was hoping for clarity -- whenever the two men get together .', '\" as soon as this meeting takes place , i will understand his attitude .', '... it would not be correct now for me to speak about what russia should do now .', '... russia must use all its possibilities in order to overcome this chaos , this terror which today is taking place in ukraine .', 'it is difficult for me to say what can come about , but i would like to say again that i am categorically against any interference with the sovereign integrity of ukraine as a state . \"', 'the location of the news conference itself underscored his isolation .', '\" i \\'m struck by the fact that he \\'s in rostov-on-don and not in moscow and that he has had a phone call with president putin and not met , \" former u.s. ambassador to russia michael mcfaul told cnn .', '\" i thought he expressed dismay that president putin has not been more decisive to defend him . \"', 'he cited yanukovych \\'s comments as \" signs that he is not in good standing with his current host . \"', 'for his part , putin discussed developments in ukraine in a telephone call with european leaders , stressing the importance of avoiding an escalation of violence , according to a statement from the kremlin , which did not cite yanukovych .', 'in a rambling news conference that lasted for more than an hour , the ousted leader spoke in russian instead of the ukrainian that he has used at official functions .', \"though he insisted that he 's still the boss and wants nothing more than to lead his country to peace , harmony and prosperity , he offered little evidence that he was in a position to do much about it .\", 'he pleaded for forgiveness \" from all who have suffered \" in the chaos and violence that have roiled the country in recent weeks .', 'he said he would \" bow down to all of them , no matter which side of the barricades they were on , \" but then unleashed a volley of epithets targeting those who were on the side of the barricades that opposed him , referring to them as \" bandits \" and \" fascists . \"', 'wearing a dark business suit and tie , he cut a lonely figure as he held forth from a table , behind which stood four blue-and-yellow ukrainian flags .', 'during the news conference , he appeared contained as he fiddled with his pen .', 'but he betrayed the pressure , staring down at his hands , which were on the table , bending a pen .', 'when it snapped , he grimaced , then looked up and continued speaking .', '\" everything that i can do , i will do to the end of my life to be with the people of ukraine -- not with those nationalists , not with those bandits , but the people of ukraine , \" he said , adding that western countries , including the united states , bore some of the blame for the unrest .', 'he said that he had left his country only after his life and the lives of his family were threatened and that he planned to return when it is safe to do so .', '\" a normal way of life must be assured to the citizens of ukraine , both in kiev and in all regions of ukraine , \" he said , and he called for a national referendum \" as soon as possible . \"', 'elections planned for may 25 in ukraine would be illegal , he said , adding that he would not participate in them if they are held .', '\" i consider that all the elections must take place in accordance with the law and constitution of ukraine , \" he said .', 'yanukovych said he was \" ashamed \" that he had not been able to maintain stability in his country .', '\" i want to apologize in front of everybody -- to the veterans , to the ukrainian people -- that i did not have the strength to stop what is now taking place in the country from taking place , \" he said .', 'but he said that \" any military action \" would be unacceptable .', '\" ukraine must remain united and undivided , \" he said , adding that the russian-majority crimean region must remain part of the state but given \" broad autonomy . \"', 'yanukovych , who is wanted in ukraine on charges related to the killings of demonstrators last week in kiev , denied having ordered police to fire on them , killing scores .', '\" no government can succeed with bloodshed , and everything that has taken place is the responsibility of those who did it , \" he said .', '\" i never gave any order for the police to shoot , \" he said , asserting that they fired in self-defense in the face of \" an attack of a mass character . \"']\n",
      "tgt_txt, len=244,ex-envoy sees signs that yanukovych \" is not in good standing with his current host \"<q>yanukovych and russian president vladimir putin have talked by phone but have not met<q>the ousted president expresses surprise that putin has not done more\n",
      "\n",
      "\n",
      "\n",
      "src, len=512,[101, 3679, 5653, 6398, 1998, 3378, 2811, 6398, 102, 101, 5890, 1024, 2340, 9765, 1010, 2538, 2255, 2286, 102, 101, 5890, 1024, 2340, 9765, 1010, 2538, 2255, 2286, 102, 101, 2471, 1020, 2454, 2402, 2111, 2024, 4445, 1999, 2082, 4496, 2551, 1010, 2429, 2000, 1037, 2817, 2207, 6928, 1012, 102, 101, 2008, 1005, 1055, 2471, 2321, 2566, 9358, 1997, 2216, 4793, 2385, 2000, 2484, 2040, 2031, 4445, 4624, 4496, 3105, 1010, 2429, 2000, 1996, 4495, 3842, 6056, 1010, 2029, 2626, 1996, 3189, 1012, 102, 101, 2060, 2913, 2031, 3491, 2008, 18373, 2402, 6001, 2024, 4394, 2041, 2006, 1037, 3332, 2000, 3857, 4813, 2027, 2097, 2342, 2101, 1999, 2166, 2030, 2224, 1996, 3716, 2027, 3734, 1999, 2267, 1012, 102, 101, 2302, 2216, 6322, 1010, 2027, 2024, 2625, 3497, 2000, 3094, 3020, 20566, 1998, 2062, 3497, 2000, 2022, 2019, 3171, 12475, 2006, 2037, 4279, 1012, 102, 101, 2502, 3291, 1024, 2471, 1020, 2454, 2402, 2111, 2024, 4445, 1999, 2082, 4496, 2551, 1010, 2429, 2000, 1037, 2817, 2207, 6928, 2011, 1996, 4495, 3842, 6056, 102, 101, 1036, 2023, 2003, 2025, 1037, 2177, 2008, 2057, 2064, 4339, 2125, 1012, 102, 101, 2027, 2074, 2342, 1037, 3382, 1010, 1005, 2056, 2928, 7380, 1010, 3237, 2472, 1997, 1996, 6056, 1997, 5661, 1010, 12288, 2967, 1010, 3343, 2180, 5705, 1998, 14495, 4411, 4056, 2000, 4852, 3171, 12969, 1012, 102, 101, 1036, 1996, 11765, 2003, 2000, 2156, 2068, 2004, 2439, 9293, 1998, 2156, 2068, 2004, 4895, 3736, 12423, 1012, 102, 101, 2021, 5278, 1996, 8790, 2003, 2025, 2183, 2000, 2022, 3733, 1012, 102, 101, 1996, 6056, 2036, 4858, 2008, 4749, 2163, 2031, 2464, 2019, 3623, 1999, 1996, 2193, 1997, 2945, 2542, 1999, 5635, 1998, 3429, 2163, 2031, 2464, 4398, 3991, 29373, 2991, 1999, 1996, 2197, 2095, 1012, 102, 101, 1996, 2079, 3126, 3189, 2104, 9363, 6072, 1996, 7860, 2402, 6001, 2227, 2085, 1998, 18921, 23567, 7860, 2027, 2024, 3497, 2000, 2227, 2004, 2027, 2131, 3080, 1012, 102, 101, 1037, 2402, 2711, 1005, 1055, 2451, 2003, 2411, 4876, 5079, 2000, 2010, 2030, 2014, 3112, 1012, 102, 101, 1036, 2027, 2074, 2342, 1037, 3382, 1010, 1005, 2758, 2928, 7380, 1010, 3237, 2472, 1997, 1996, 6056, 1997, 5661, 1010, 12288, 2967, 1010, 3343, 2180, 5705, 1998, 14495, 4411, 4056, 2000, 4852, 3171, 12969, 102, 101, 1996, 4495, 3842, 3189, 12808, 2385, 5876, 1011, 4274, 3229, 1010, 2267, 7665, 6165, 1010, 3318, 16440, 1998, 2270, 3808, 2426, 2068, 1011, 1998, 4453, 2163, 2008, 2020, 2725, 2092, 2005, 2049, 2402, 2111, 1012, 102, 101, 22286, 1996, 2862, 1997, 16408, 2163, 2024, 8839, 1010, 5135, 1998, 2167, 7734, 1012, 102, 101, 7756, 1010, 5900, 1998, 2047, 3290, 1012, 102, 101, 2451, 1005, 1055, 6346, 1024, 4918, 2158, 10440, 17080, 1010, 2040, 2573, 2007, 2095, 2039, 1010, 1037, 14495, 2008, 4499, 2402, 6001, 2005, 10922, 1998, 7126, 2068, 2424, 5841, 1010, 2758, 2037, 10461, 2003, 1036, 2205, 2411, 4340, 2011, 2037, 14101, 3642, 1005, 102, 101, 1036, 2037, 10461, 2003, 2205, 2411, 4340, 2011, 2037, 14101, 3642, 1010, 1005, 2056, 4918, 2158, 10440, 17080, 1010, 2040, 2573, 2007, 2095, 2039, 1010, 1037, 14495, 2008, 4499, 2402, 102]\n",
      "labels, len=21,[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "segs, len=512,[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "clss, len=21,[0, 9, 19, 29, 50, 84, 118, 144, 172, 186, 222, 242, 255, 290, 319, 337, 373, 410, 426, 435, 480]\n",
      "src_txt, len=51,['daily mail reporter and associated press reporter', '01:11 est , 21 october 2013', '01:11 est , 21 october 2013', 'almost 6 million young people are neither in school nor working , according to a study released monday .', \"that 's almost 15 per cent of those aged 16 to 24 who have neither desk nor job , according to the opportunity nation coalition , which wrote the report .\", 'other studies have shown that idle young adults are missing out on a window to build skills they will need later in life or use the knowledge they acquired in college .', 'without those experiences , they are less likely to command higher salaries and more likely to be an economic drain on their communities .', 'big problem : almost 6 million young people are neither in school nor working , according to a study released monday by the opportunity nation coalition', '` this is not a group that we can write off .', \"they just need a chance , ' said mark edwards , executive director of the coalition of businesses , advocacy groups , policy wonks and nonprofit organizations dedicated to increasing economic mobility .\", '` the tendency is to see them as lost souls and see them as unsavable .', 'but changing the dynamic is not going to be easy .', 'the coalition also finds that 49 states have seen an increase in the number of families living in poverty and 45 states have seen household median incomes fall in the last year .', 'the dour report underscores the challenges young adults face now and foretell challenges they are likely to face as they get older .', \"a young person 's community is often closely tied to his or her success .\", \"` they just need a chance , ' says mark edwards , executive director of the coalition of businesses , advocacy groups , policy wonks and nonprofit organizations dedicated to increasing economic mobility\", 'the opportunity nation report tracked 16 factors - internet access , college graduation rates , income inequality and public safety among them - and identified states that were doing well for its young people .', 'topping the list of supportive states are vermont , minnesota and north dakota .', 'nevada , mississippi and new mexico .', \"community 's fault : charlie mangiardi , who works with year up , a nonprofit that trains young adults for careers and helps them find jobs , says their destiny is ` too often determined by their zip code '\", \"` their destiny is too often determined by their zip code , ' said charlie mangiardi , who works with year up , a nonprofit that trains young adults for careers and helps them find jobs .\", '` we have the supply .', \"we do n't have a lack of young people who need this opportunity , ' mangiardi added .\", \"just look at some of the nation 's largest cities .\", 'chicago , houston , dallas , miami , philadelphia , new york , los angeles , atlanta and riverside , california , all have more than 100,000 idle youth , the opportunity nation report found .', \"` often times they lack the social capital in life , ' mangiardi said .\", \"` there 's a whole pool of talent that is motivated , loyal and hardworking . '\", \"they just ca n't get through an employer 's door , he added .\", \"that 's why year up spends a year working with high school graduates to teach them career skills such as computer programming or equipment repair they can use when the program ends .\", 'it also includes life coaching so they can learn skills such as time management .', 'more than 4,500 young adults from urban areas have completed the program and 84 per cent of them have found work .', \"nothing to do : some of the nation 's largest cities .\", 'chicago , houston , dallas , miami , philadelphia , new york , los angeles , atlanta and riverside , california , all have more than 100,000 idle youth', 'across the country : 49 states have seen an increase in the number of families living in poverty and 45 states have seen household median incomes fall in the last year', \"but it 's a far tougher time for other young people .\", 'in mississippi and west virginia , 1 in 5 young people are idle - higher than their older neighbors .', 'mississippi has an overall unemployment rate of 8 per cent , while west virginia posts about 7 per cent .', 'like most states , they saw their unemployment rate fall since 2011 , but researchers caution that shift could come from fewer residents looking for work and from more who had simply given up their search for jobs .', 'way forward : rob denson , president of des moines area community college in iowa , has helped rally community organizations in his city to develop a pilot program to help students as young as 14 find summer work', \"and it 's not as though the challenges emerge from nowhere .\", \"quality early childhood programs help students from poor families overcome societal hurdles , and on-time high school graduation rates often follow quality schools - other factors opportunity nation examined in its report . '\", \"a lot of times we do n't want to look at data because we do n't want to be depressed , ' said rob denson , president of des moines area community college in iowa .\", \"but it 's an uncomfortable reality that needs to be addressed , he said .\", \"using previous years ' reports from opportunity nation , denson helped rally community organizations in his city to develop a pilot program to help students as young as 14 find summer work .\", 'starting next summer , des moines students will be placed in paying jobs , part of a citywide collaboration to help its urban communities .', 'it will help older adults , as well , because crime rates are expected to fall , he said .', \"` if they 're not in school or at work , they 're not usually doing something positive . '\", 'have seen increase in the number of families living in poverty', 'states have seen household median incomes fall in the last year', 'philadelphia , new york , los angeles , atlanta and riverside , california , all', 'have more than 100,000 idle youth']\n",
      "tgt_txt, len=685,15 per cent of those aged 16 to 24 who have neither desk nor job , according to report by the the opportunity nation coalition<q>idle young adults are missing out on a window to build skills they will need later in life<q>49 states<q>45<q>a young person 's community is often closely tied to his or her success<q>states that were doing well for its young people include vermont , minnesota and north dakota .<q>nevada , mississippi and new mexico are at the bottom of the list<q>chicago , houston , dallas , miami ,<q>in mississippi and west virginia 1 in 5 young people are idle<q>mississippi has an overall unemployment rate of 8 per cent , while west virginia posts about 7 per cent\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loadding the processed data from BertSum github\n",
    "# We're going to process our own data format for better control\n",
    "# and simplify the process using Torch dataset and dataloader\n",
    "sample_path = './dataset/bertsum_data/cnndm.train.0.bert.pt'\n",
    "\n",
    "assert os.path.exists(sample_path), \"Path does not exist\"\n",
    "\n",
    "dataset = torch.load(sample_path)\n",
    "print(type(dataset))\n",
    "\n",
    "for data in dataset[:2]:\n",
    "    for key, value in data.items():\n",
    "        print(f'{key}, len={len(value)},{value}')\n",
    "    print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n",
       "      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n",
       "      <td>Criminal complaint: Cop used his role to help ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A drunk driver who killed a young woman in a h...</td>\n",
       "      <td>Craig Eccleston-Todd, 27, had drunk at least t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(CNN) -- With a breezy sweep of his pen Presid...</td>\n",
       "      <td>Nina dos Santos says Europe must be ready to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fleetwood are the only team still to have a 10...</td>\n",
       "      <td>Fleetwood top of League One after 2-0 win at S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
       "1  (CNN) -- Ralph Mata was an internal affairs li...   \n",
       "2  A drunk driver who killed a young woman in a h...   \n",
       "3  (CNN) -- With a breezy sweep of his pen Presid...   \n",
       "4  Fleetwood are the only team still to have a 10...   \n",
       "\n",
       "                                          highlights  \n",
       "0  Bishop John Folda, of North Dakota, is taking ...  \n",
       "1  Criminal complaint: Cop used his role to help ...  \n",
       "2  Craig Eccleston-Todd, 27, had drunk at least t...  \n",
       "3  Nina dos Santos says Europe must be ready to a...  \n",
       "4  Fleetwood top of League One after 2-0 win at S...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read and process cnn/dm data\n",
    "root_path = './dataset/cnn_dailymail/train.csv'\n",
    "\n",
    "#only read first 100 rows for quick development\n",
    "train_data = pd.read_csv(root_path, nrows=100)\n",
    "\n",
    "train_data = train_data.drop(['id'], axis=1)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize each article and highlights and build vocab\n",
    "\n",
    "1. Clean\n",
    "2. Expand contractions\n",
    "3. Remove stopwords\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bishop', 'john', 'folda', 'of', 'north', 'dakota', 'is', 'taking', 'time', 'off', 'after', 'being', 'diagnosed', '', '', 'he', 'contracted', 'the', 'infection', 'through', 'contaminated', 'food', 'in', 'italy', '', '', 'church', 'members', 'in', 'fargo', 'grand', 'forks', 'and', 'jamestown', 'could', 'have', 'been', 'exposed', '']\n",
      "sents =  [['bishop', 'fargo', 'catholic', 'diocese', 'north', 'dakota', 'exposed', 'potentially', 'hundreds', 'church', 'members', 'fargo', 'grand', 'forks', 'jamestown', 'hepatitis', 'virus', 'late', 'september', 'early', 'october'], ['state', 'health', 'department', 'issued', 'advisory', 'exposure', 'anyone', 'attended', 'five', 'churches', 'took', 'communion'], ['bishop', 'john', 'folda', 'pictured', 'fargo', 'catholic', 'diocese', 'north', 'dakota', 'exposed', 'potentially', 'hundreds', 'church', 'members', 'fargo', 'grand', 'forks', 'jamestown', 'hepatitis'], ['state', 'immunization', 'program', 'manager', 'molly', 'howell', 'says', 'risk', 'low', 'officials', 'feel', 'important', 'alert', 'people', 'possible', 'exposure'], ['diocese', 'announced', 'monday', 'bishop', 'john', 'folda', 'taking', 'time', 'diagnosed', 'hepatitis', 'a'], ['diocese', 'says', 'contracted', 'infection', 'contaminated', 'food', 'attending', 'conference', 'newly', 'ordained', 'bishops', 'italy', 'last', 'month'], ['symptoms', 'hepatitis', 'include', 'fever', 'tiredness', 'loss', 'appetite', 'nausea', 'abdominal', 'discomfort'], ['fargo', 'catholic', 'diocese', 'north', 'dakota', 'pictured', 'bishop', 'located']]\n",
      "abstract =  [['bishop', 'fargo', 'catholic', 'diocese', 'north', 'dakota', 'exposed', 'potentially', 'hundreds', 'church', 'members', 'fargo', 'grand', 'forks', 'jamestown', 'hepatitis', 'virus', 'late', 'september', 'early', 'october'], ['state', 'health', 'department', 'issued', 'advisory', 'exposure', 'anyone', 'attended', 'five', 'churches', 'took', 'communion'], ['bishop', 'john', 'folda', 'pictured', 'fargo', 'catholic', 'diocese', 'north', 'dakota', 'exposed', 'potentially', 'hundreds', 'church', 'members', 'fargo', 'grand', 'forks', 'jamestown', 'hepatitis'], ['state', 'immunization', 'program', 'manager', 'molly', 'howell', 'says', 'risk', 'low', 'officials', 'feel', 'important', 'alert', 'people', 'possible', 'exposure'], ['diocese', 'announced', 'monday', 'bishop', 'john', 'folda', 'taking', 'time', 'diagnosed', 'hepatitis', 'a'], ['diocese', 'says', 'contracted', 'infection', 'contaminated', 'food', 'attending', 'conference', 'newly', 'ordained', 'bishops', 'italy', 'last', 'month'], ['symptoms', 'hepatitis', 'include', 'fever', 'tiredness', 'loss', 'appetite', 'nausea', 'abdominal', 'discomfort'], ['fargo', 'catholic', 'diocese', 'north', 'dakota', 'pictured', 'bishop', 'located']]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"str\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 105\u001b[0m\n\u001b[0;32m    100\u001b[0m     labels[i] \u001b[39m=\u001b[39m labels[i]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    101\u001b[0m \u001b[39m# \u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39m# print(sent)\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[39m# print(labels)\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m \u001b[39mprint\u001b[39m(greedy_selection(sent, labels, \u001b[39m3\u001b[39;49m))\n",
      "Cell \u001b[1;32mIn[21], line 63\u001b[0m, in \u001b[0;36mgreedy_selection\u001b[1;34m(doc_sent_list, abstract_sent_list, summary_size)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39msents = \u001b[39m\u001b[39m'\u001b[39m, sents)\n\u001b[0;32m     61\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mabstract = \u001b[39m\u001b[39m'\u001b[39m, sents)\n\u001b[1;32m---> 63\u001b[0m evaluated_1grams \u001b[39m=\u001b[39m [_get_word_ngrams(\u001b[39m1\u001b[39m, [sent]) \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sents]\n\u001b[0;32m     64\u001b[0m reference_1grams \u001b[39m=\u001b[39m _get_word_ngrams(\u001b[39m1\u001b[39m, [abstract])\n\u001b[0;32m     65\u001b[0m evaluated_2grams \u001b[39m=\u001b[39m [_get_word_ngrams(\u001b[39m2\u001b[39m, [sent]) \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sents]\n",
      "Cell \u001b[1;32mIn[21], line 63\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39msents = \u001b[39m\u001b[39m'\u001b[39m, sents)\n\u001b[0;32m     61\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mabstract = \u001b[39m\u001b[39m'\u001b[39m, sents)\n\u001b[1;32m---> 63\u001b[0m evaluated_1grams \u001b[39m=\u001b[39m [_get_word_ngrams(\u001b[39m1\u001b[39;49m, [sent]) \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sents]\n\u001b[0;32m     64\u001b[0m reference_1grams \u001b[39m=\u001b[39m _get_word_ngrams(\u001b[39m1\u001b[39m, [abstract])\n\u001b[0;32m     65\u001b[0m evaluated_2grams \u001b[39m=\u001b[39m [_get_word_ngrams(\u001b[39m2\u001b[39m, [sent]) \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sents]\n",
      "Cell \u001b[1;32mIn[21], line 46\u001b[0m, in \u001b[0;36m_get_word_ngrams\u001b[1;34m(n, sentences)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39massert\u001b[39;00m n \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     44\u001b[0m \u001b[39m# words = _split_into_words(sentences)\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m words \u001b[39m=\u001b[39m \u001b[39msum\u001b[39;49m([\u001b[39m'\u001b[39;49m\u001b[39msomething here\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mnew sent\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39msetn2\u001b[39;49m\u001b[39m'\u001b[39;49m], [])\n\u001b[0;32m     47\u001b[0m \u001b[39m# words = [w for w in words if w not in stopwords]\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39mreturn\u001b[39;00m _get_ngrams(n, words)\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate list (not \"str\") to list"
     ]
    }
   ],
   "source": [
    "#Trying to understand greedy selection using ngrams\n",
    "# https://github.com/nlpyang/BertSum/blob/05f8c634197d0ed1be8157d71f29aa7765abdd2a/src/prepro/data_builder.py#L43\n",
    "def cal_rouge(evaluated_ngrams, reference_ngrams):\n",
    "    reference_count = len(reference_ngrams)\n",
    "    evaluated_count = len(evaluated_ngrams)\n",
    "\n",
    "    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
    "    overlapping_count = len(overlapping_ngrams)\n",
    "\n",
    "    if evaluated_count == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = overlapping_count / evaluated_count\n",
    "\n",
    "    if reference_count == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = overlapping_count / reference_count\n",
    "\n",
    "    f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
    "    return {\"f\": f1_score, \"p\": precision, \"r\": recall}\n",
    "\n",
    "def _get_ngrams(n, text):\n",
    "    \"\"\"Calcualtes n-grams.\n",
    "    Args:\n",
    "      n: which n-grams to calculate\n",
    "      text: An array of tokens\n",
    "    Returns:\n",
    "      A set of n-grams\n",
    "    \"\"\"\n",
    "    ngram_set = set()\n",
    "    text_length = len(text)\n",
    "    max_index_ngram_start = text_length - n\n",
    "    for i in range(max_index_ngram_start + 1):\n",
    "        ngram_set.add(tuple(text[i:i + n]))\n",
    "    return ngram_set\n",
    "\n",
    "def _get_word_ngrams(n, sentences):\n",
    "    \"\"\"Calculates word n-grams for multiple sentences.\n",
    "    \"\"\"\n",
    "    assert len(sentences) > 0\n",
    "    assert n > 0\n",
    "\n",
    "    # words = _split_into_words(sentences)\n",
    "\n",
    "    words = sum(['something here', 'new sent', 'setn2'], [])\n",
    "    # words = [w for w in words if w not in stopwords]\n",
    "    return _get_ngrams(n, words)\n",
    "\n",
    "def greedy_selection(doc_sent_list, abstract_sent_list, summary_size):\n",
    "    def _rouge_clean(s):\n",
    "        return re.sub(r'[^a-zA-Z0-9 ]', '', s)\n",
    "\n",
    "    max_rouge = 0.0\n",
    "    abstract = sum(abstract_sent_list, [])\n",
    "    print(abstract)\n",
    "    abstract = _rouge_clean(' '.join(abstract)).split()\n",
    "    sents = [_rouge_clean(' '.join(s)).split() for s in doc_sent_list]\n",
    "\n",
    "    print('sents = ', sents)\n",
    "    print('abstract = ', sents)\n",
    "\n",
    "    return\n",
    "\n",
    "    evaluated_1grams = [_get_word_ngrams(1, [sent]) for sent in sents]\n",
    "    reference_1grams = _get_word_ngrams(1, [abstract])\n",
    "    evaluated_2grams = [_get_word_ngrams(2, [sent]) for sent in sents]\n",
    "    reference_2grams = _get_word_ngrams(2, [abstract])\n",
    "\n",
    "    selected = []\n",
    "    for s in range(summary_size):\n",
    "        cur_max_rouge = max_rouge\n",
    "        cur_id = -1\n",
    "        for i in range(len(sents)):\n",
    "            if (i in selected):\n",
    "                continue\n",
    "            c = selected + [i]\n",
    "            candidates_1 = [evaluated_1grams[idx] for idx in c]\n",
    "            candidates_1 = set.union(*map(set, candidates_1))\n",
    "            candidates_2 = [evaluated_2grams[idx] for idx in c]\n",
    "            candidates_2 = set.union(*map(set, candidates_2))\n",
    "            rouge_1 = cal_rouge(candidates_1, reference_1grams)['f']\n",
    "            rouge_2 = cal_rouge(candidates_2, reference_2grams)['f']\n",
    "            rouge_score = rouge_1 + rouge_2\n",
    "            if rouge_score > cur_max_rouge:\n",
    "                cur_max_rouge = rouge_score\n",
    "                cur_id = i\n",
    "        if (cur_id == -1):\n",
    "            return selected\n",
    "        selected.append(cur_id)\n",
    "        max_rouge = cur_max_rouge\n",
    "\n",
    "    return sorted(selected)\n",
    "\n",
    "sent = [' bishop fargo catholic diocese north dakota exposed potentially hundreds church members fargo grand forks jamestown hepatitis virus late september early october', ' state health department issued advisory exposure anyone attended five churches took communion', ' bishop john folda pictured fargo catholic diocese north dakota exposed potentially hundreds church members fargo grand forks jamestown hepatitis ', ' state immunization program manager molly howell says risk low officials feel important alert people possible exposure', ' diocese announced monday bishop john folda taking time diagnosed hepatitis a', ' diocese says contracted infection contaminated food attending conference newly ordained bishops italy last month', ' symptoms hepatitis include fever tiredness loss appetite nausea abdominal discomfort', ' fargo catholic diocese north dakota pictured bishop located ']\n",
    "labels = ['bishop john folda of north dakota is taking time off after being diagnosed ', ' he contracted the infection through contaminated food in italy ', ' church members in fargo grand forks and jamestown could have been exposed ']\n",
    "\n",
    "for i in range(len(sent)):\n",
    "    sent[i] = sent[i].split(' ')\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    labels[i] = labels[i].split(' ')\n",
    "# \n",
    "# print(sent)\n",
    "# print(labels)\n",
    "\n",
    "print(greedy_selection(sent, labels, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: https://www.kaggle.com/code/mohamedaref000/seq2seq-enc-dec\n",
    "def clean_text(text, remove_stopwords=True):\n",
    "    '''\n",
    "    @args:\n",
    "        text: string, raw text\n",
    "        remove_stopwords: boolean, default = False\n",
    "    @return\n",
    "        []: list of sentences\n",
    "\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    tmp = []\n",
    "    for word in text:\n",
    "        if word in contractions:\n",
    "            tmp.append(contractions[word])\n",
    "        else:\n",
    "            tmp.append(word)\n",
    "    text = ' '.join(tmp)\n",
    "    \n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    #“’‘–»\n",
    "    #NOTE that we're keep dot to seperate sentences for BERTSUM\n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%,!?:#$@\\[\\]/”]', '', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    #NOTE: remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words('english'))\n",
    "        text = [w for w in text if w not in stops]\n",
    "        text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "#Process text into the format BERTSUM requires, read the paper\n",
    "# NOTE that min_len filter sentences with fewer words than it, set to 10 for now but maybe \n",
    "# there's a better number\n",
    "def process_text(text, min_len = 5, remove_stopwords = True):\n",
    "    '''\n",
    "    @args\n",
    "        text: string, raw text\n",
    "        min_len: int, default = 5, if sentence has fewer words than min_len, ignore it\n",
    "    @returns\n",
    "        sentences: [], list of sentences \n",
    "    '''\n",
    "    cleaned = clean_text(text, remove_stopwords)\n",
    "    sentences = cleaned.split('.')\n",
    "    sentences = list(filter(lambda x: len(x.split()) > min_len, sentences))\n",
    "    #strip the spaces\n",
    "    for i in sentences:\n",
    "        i = i.strip()\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample data after processed\n",
      "[' bishop fargo catholic diocese north dakota exposed potentially hundreds church members fargo grand forks jamestown hepatitis virus late september early october', ' state health department issued advisory exposure anyone attended five churches took communion', ' bishop john folda pictured fargo catholic diocese north dakota exposed potentially hundreds church members fargo grand forks jamestown hepatitis ', ' state immunization program manager molly howell says risk low officials feel important alert people possible exposure', ' diocese announced monday bishop john folda taking time diagnosed hepatitis a', ' diocese says contracted infection contaminated food attending conference newly ordained bishops italy last month', ' symptoms hepatitis include fever tiredness loss appetite nausea abdominal discomfort', ' fargo catholic diocese north dakota pictured bishop located ']\n",
      "original article:  By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A . State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. The diocese announced on Monday that Bishop John Folda is taking time off after being diagnosed with hepatitis A. The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located .\n",
      "highlights\n",
      "['bishop john folda of north dakota is taking time off after being diagnosed ', ' he contracted the infection through contaminated food in italy ', ' church members in fargo grand forks and jamestown could have been exposed ']\n",
      "original highlights:  Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\n",
      "He contracted the infection through contaminated food in Italy .\n",
      "Church members in Fargo, Grand Forks and Jamestown could have been exposed .\n"
     ]
    }
   ],
   "source": [
    "cleaned_highlights = []\n",
    "cleaned_articles = []\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "    cleaned_articles.append(process_text(row['article']))\n",
    "    cleaned_highlights.append(process_text(row['highlights'], remove_stopwords=False))\n",
    "\n",
    "print('sample data after processed')\n",
    "print(cleaned_articles[0])\n",
    "print('original article: ', train_data.article[0])\n",
    "print('highlights')\n",
    "print(cleaned_highlights[0])\n",
    "print('original highlights: ', train_data.highlights[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocab and write it to vocab file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data(articles):\n",
    "    tokenizer = torchtext.data.get_tokenizer('spacy')\n",
    "    tokens = []\n",
    "\n",
    "    for article in articles:\n",
    "        tokens += tokenizer(' '.join(article))\n",
    "    \n",
    "    #NOTE word freq can affect performance down stream. Put attention to this param\n",
    "    vocab = Vocab(tokens, 3, reserved_tokens=['<cls>', '<sep>'])\n",
    "    return tokenizer, vocab\n",
    "\n",
    "def batchify(articles, highlights, max_len = 512):\n",
    "\n",
    "    tokenizer, vocab = _load_data(articles)\n",
    "\n",
    "    def _save_vocab(vocab):\n",
    "        output_root = './output'\n",
    "        processed_root = os.path.join(output_root, 'processed')\n",
    "        if os.path.exists(processed_root) == False: \n",
    "            os.makedirs(processed_root)\n",
    "        vocab_path = os.path.join(processed_root, 'vocab.txt')\n",
    "        vocab.write_to(vocab_path)\n",
    "\n",
    "    def _get_src_segments_labels(vocab, article: list):\n",
    "        assert type(article) == list, 'article must be list of string'\n",
    "        src = []\n",
    "        segs = []\n",
    "        labels =[]\n",
    "        clss = []\n",
    "\n",
    "        for sentence in article:\n",
    "            sent_tokens = vocab[sentence]\n",
    "            src.extend(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "def test(a: list):\n",
    "    assert type(a) == list, 'not correct type'\n",
    "    print('ok')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len =  3170\n",
      "[310, 1039, 453]\n"
     ]
    }
   ],
   "source": [
    "vocab = _load_data(cleaned_articles)\n",
    "\n",
    "print('vocab len = ', len(vocab))\n",
    "sample_sentence = 'bishop fargo catholic'\n",
    "print(vocab[sample_sentence.split(' ')])\n",
    "\n",
    "output_root = './output'\n",
    "processed_root = os.path.join(output_root, 'processed')\n",
    "\n",
    "if os.path.exists(processed_root) == False: \n",
    "    os.makedirs(processed_root)\n",
    "\n",
    "vocab_path = os.path.join(processed_root, 'vocab.txt')\n",
    "\n",
    "vocab.write_to(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDailyMailDataset(Dataset):\n",
    "    def __init__(self, path, type = 'train'):\n",
    "        self.path = path\n",
    "        self.type = type\n",
    "\n",
    "        assert type in ['train', 'valid', 'test'], \"dataset type must be train, valid, or test\"\n",
    "\n",
    "        self.src = []\n",
    "        self.labels = []\n",
    "        self.segments = []\n",
    "        self.cls = []\n",
    "        self.src_txt = []\n",
    "        self.tgt_txt = []\n",
    "\n",
    "        self._load_cnndm()\n",
    "        #Load all data into list\n",
    "    \n",
    "    def _truncate_or_pad(self, data, max_len):\n",
    "        '''\n",
    "        @args\n",
    "            data: list of all data\n",
    "            max_len: max_length for each data\n",
    "        @return\n",
    "        '''\n",
    "\n",
    "        src_list, segments_list, labels_list, cls_list, src_txt_list, tgt_txt_list = [], [], [], [], [], []\n",
    "        for src, labels, segs, clsss, src_txt, tgt_txt in data:\n",
    "            src_list.append(torch.tensor(src ))\n",
    "\n",
    "\n",
    "\n",
    "    def clear_data(self):\n",
    "        self.src.clear()\n",
    "        self.labels.clear()\n",
    "        self.segments.clear()\n",
    "        self.cls.clear()\n",
    "        self.src_txt.clear()\n",
    "        self.tgt_txt.clear()\n",
    "\n",
    "    def _load_cnndm(self):\n",
    "        assert os.path.exists(self.path), \"Path to CNN/DailyMail dataset can't be found\"\n",
    "\n",
    "        self.clear_data()\n",
    "\n",
    "        self.src = []\n",
    "        self.labels = []\n",
    "        self.segments = []\n",
    "        self.cls = []\n",
    "        self.src_txt = []\n",
    "        self.tgt_txt = []\n",
    "\n",
    "        if self.type == 'train':\n",
    "            chunks = glob.glob(f'{self.path}/cnndm.train.*.bert.pt')\n",
    "        elif self.type == 'test':\n",
    "            chunks = glob.glob(f'{self.path}/cnndm.test.*.bert.pt')\n",
    "        else:\n",
    "            chunks = glob.glob(f'{self.path}/cnndm.valid.*.bert.pt')\n",
    "\n",
    "        tmp = []\n",
    "        for chunk in chunks:\n",
    "            d = torch.load(chunk)\n",
    "            tmp.append(d)\n",
    "            #Debug: work on 1 chunk first, then load all of them\n",
    "            break\n",
    "    \n",
    "        #truncate and pad if needed\n",
    "        (self.src, \n",
    "         self.labels,\n",
    "         self.segments,\n",
    "         self.cls, \n",
    "         self.src_txt, \n",
    "         self.tgt_txt) = self._truncate_or_pad(tmp, 512)\n",
    "\n",
    "        print(f'loaded {len(self.data)}')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 2001\n"
     ]
    }
   ],
   "source": [
    "train_set = CNNDailyMailDataset(root_path, 'train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key=src, len=512, value=[101, 2002, 1005, 1055, 2006, 1996, 2648, 2559, 1999, 1010, 1998, 1996, 3193, 1997, 13489, 13619, 6968, 4492, 17994, 3849, 2000, 2562, 2893, 11737, 5017, 1012, 102, 101, 1037, 2739, 3034, 5958, 2013, 2010, 2047, 7728, 1999, 8252, 3607, 2104, 9363, 5596, 2074, 2129, 11737, 2010, 16746, 3711, 2000, 2022, 1010, 2004, 1996, 28368, 5969, 2343, 10865, 2008, 2010, 3677, 1011, 1011, 1998, 4022, 27398, 1011, 1011, 2001, 2025, 2105, 1998, 2018, 2589, 2210, 2000, 2191, 2010, 2994, 2062, 6625, 1012, 102, 101, 1000, 1045, 5136, 2008, 3607, 2442, 1998, 2038, 2000, 2552, 1010, 1000, 13619, 6968, 4492, 17994, 2409, 1037, 6887, 7911, 26807, 1997, 12060, 2040, 2018, 9240, 1999, 1996, 2103, 1997, 20996, 29473, 1011, 2006, 1011, 2123, 1010, 2379, 1996, 8772, 3675, 2007, 5924, 1998, 2055, 6352, 2661, 2148, 1997, 4924, 1012, 102, 101, 2002, 2106, 2025, 20648, 2054, 4506, 2002, 2001, 5327, 2005, 2021, 2081, 3154, 3183, 2002, 2001, 2559, 2000, 1012, 102, 101, 1000, 4209, 1996, 2839, 1997, 2720, 1012, 8748, 22072, 1010, 1045, 2572, 4527, 2339, 1010, 2039, 1005, 18681, 2085, 1010, 2002, 2038, 2815, 19868, 1010, 4333, 1012, 102, 101, 2008, 2003, 1996, 3160, 1012, 1000, 102, 101, 2295, 2002, 1998, 1996, 2845, 2343, 2018, 5720, 2058, 1996, 3042, 1010, 2027, 2018, 2025, 2777, 1010, 2056, 13619, 6968, 4492, 17994, 1010, 2040, 2596, 2000, 9530, 22119, 2008, 2002, 2106, 2025, 2113, 2073, 22072, 4832, 2006, 2010, 15068, 6238, 1998, 2001, 5327, 2005, 15563, 1011, 1011, 7188, 1996, 2048, 2273, 2131, 2362, 1012, 102, 101, 1000, 2004, 2574, 2004, 2023, 3116, 3138, 2173, 1010, 1045, 2097, 3305, 2010, 7729, 1012, 102, 101, 1012, 1012, 1012, 2009, 2052, 2025, 2022, 6149, 2085, 2005, 2033, 2000, 3713, 2055, 2054, 3607, 2323, 2079, 2085, 1012, 102, 101, 1012, 1012, 1012, 3607, 2442, 2224, 2035, 2049, 12020, 1999, 2344, 2000, 9462, 2023, 8488, 1010, 2023, 7404, 2029, 2651, 2003, 2635, 2173, 1999, 5924, 1012, 102, 101, 2009, 2003, 3697, 2005, 2033, 2000, 2360, 2054, 2064, 2272, 2055, 1010, 2021, 1045, 2052, 2066, 2000, 2360, 2153, 2008, 1045, 2572, 4937, 27203, 2135, 2114, 2151, 11099, 2007, 1996, 11074, 11109, 1997, 5924, 2004, 1037, 2110, 1012, 1000, 102, 101, 1996, 3295, 1997, 1996, 2739, 3034, 2993, 2104, 9363, 5596, 2010, 12477, 1012, 102, 101, 1000, 1045, 1005, 1049, 4930, 2011, 1996, 2755, 2008, 2002, 1005, 1055, 1999, 20996, 29473, 1011, 2006, 1011, 2123, 1998, 2025, 1999, 4924, 1998, 2008, 2002, 2038, 2018, 1037, 3042, 2655, 2007, 2343, 22072, 1998, 2025, 2777, 1010, 1000, 2280, 1057, 1012, 1055, 1012, 6059, 2000, 3607, 2745, 11338, 7011, 5313, 2409, 13229, 1012, 102, 101, 1000, 1045, 2245, 2002, 5228, 20006, 2008, 2343, 22072, 2038, 2025, 2042, 2062, 13079, 2000, 6985, 2032, 1012, 1000, 102, 101, 2002, 6563, 13619, 6968, 4492, 17994, 1005, 1055, 7928, 2004, 1000, 5751, 2008, 2002, 2003, 2025, 1999, 2204, 3061, 2007, 2010, 2783, 3677, 1012, 1000, 102, 101, 2005, 2010, 2112, 1010, 22072, 6936, 8973, 1999, 5924, 1999, 1037, 7026, 2655, 2007, 2647, 4177, 1010, 6911, 2075, 1996, 5197, 1997, 9992, 2019, 9686, 25015, 3508, 1997, 4808, 1010, 2429, 2000, 1037, 4861, 102]\n",
      "key=labels, len=16, value=[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]\n",
      "key=segs, len=512, value=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "key=clss, len=16, value=[0, 27, 82, 135, 156, 185, 193, 249, 266, 288, 316, 357, 372, 428, 449, 476]\n",
      "key=src_txt, len=36, value=[\"he 's on the outside looking in , and the view of viktor yanukovych seems to keep getting dimmer .\", 'a news conference friday from his new quarters in southeastern russia underscored just how dim his prospects appear to be , as the ousted ukrainian president complained that his host -- and potential benefactor -- was not around and had done little to make his stay more comfortable .', '\" i consider that russia must and has to act , \" yanukovych told a phalanx of reporters who had assembled in the city of rostov-on-don , near the southwestern border with ukraine and about 700 miles south of moscow .', 'he did not specify what actions he was hoping for but made clear whom he was looking to .', '\" knowing the character of mr. vladimir putin , i am surprised why , up \\'til now , he has remained restrained , silent .', 'that is the question . \"', 'though he and the russian president had talked over the phone , they had not met , said yanukovych , who appeared to concede that he did not know where putin stands on his ouster and was hoping for clarity -- whenever the two men get together .', '\" as soon as this meeting takes place , i will understand his attitude .', '... it would not be correct now for me to speak about what russia should do now .', '... russia must use all its possibilities in order to overcome this chaos , this terror which today is taking place in ukraine .', 'it is difficult for me to say what can come about , but i would like to say again that i am categorically against any interference with the sovereign integrity of ukraine as a state . \"', 'the location of the news conference itself underscored his isolation .', '\" i \\'m struck by the fact that he \\'s in rostov-on-don and not in moscow and that he has had a phone call with president putin and not met , \" former u.s. ambassador to russia michael mcfaul told cnn .', '\" i thought he expressed dismay that president putin has not been more decisive to defend him . \"', 'he cited yanukovych \\'s comments as \" signs that he is not in good standing with his current host . \"', 'for his part , putin discussed developments in ukraine in a telephone call with european leaders , stressing the importance of avoiding an escalation of violence , according to a statement from the kremlin , which did not cite yanukovych .', 'in a rambling news conference that lasted for more than an hour , the ousted leader spoke in russian instead of the ukrainian that he has used at official functions .', \"though he insisted that he 's still the boss and wants nothing more than to lead his country to peace , harmony and prosperity , he offered little evidence that he was in a position to do much about it .\", 'he pleaded for forgiveness \" from all who have suffered \" in the chaos and violence that have roiled the country in recent weeks .', 'he said he would \" bow down to all of them , no matter which side of the barricades they were on , \" but then unleashed a volley of epithets targeting those who were on the side of the barricades that opposed him , referring to them as \" bandits \" and \" fascists . \"', 'wearing a dark business suit and tie , he cut a lonely figure as he held forth from a table , behind which stood four blue-and-yellow ukrainian flags .', 'during the news conference , he appeared contained as he fiddled with his pen .', 'but he betrayed the pressure , staring down at his hands , which were on the table , bending a pen .', 'when it snapped , he grimaced , then looked up and continued speaking .', '\" everything that i can do , i will do to the end of my life to be with the people of ukraine -- not with those nationalists , not with those bandits , but the people of ukraine , \" he said , adding that western countries , including the united states , bore some of the blame for the unrest .', 'he said that he had left his country only after his life and the lives of his family were threatened and that he planned to return when it is safe to do so .', '\" a normal way of life must be assured to the citizens of ukraine , both in kiev and in all regions of ukraine , \" he said , and he called for a national referendum \" as soon as possible . \"', 'elections planned for may 25 in ukraine would be illegal , he said , adding that he would not participate in them if they are held .', '\" i consider that all the elections must take place in accordance with the law and constitution of ukraine , \" he said .', 'yanukovych said he was \" ashamed \" that he had not been able to maintain stability in his country .', '\" i want to apologize in front of everybody -- to the veterans , to the ukrainian people -- that i did not have the strength to stop what is now taking place in the country from taking place , \" he said .', 'but he said that \" any military action \" would be unacceptable .', '\" ukraine must remain united and undivided , \" he said , adding that the russian-majority crimean region must remain part of the state but given \" broad autonomy . \"', 'yanukovych , who is wanted in ukraine on charges related to the killings of demonstrators last week in kiev , denied having ordered police to fire on them , killing scores .', '\" no government can succeed with bloodshed , and everything that has taken place is the responsibility of those who did it , \" he said .', '\" i never gave any order for the police to shoot , \" he said , asserting that they fired in self-defense in the face of \" an attack of a mass character . \"']\n",
      "key=tgt_txt, len=244, value=ex-envoy sees signs that yanukovych \" is not in good standing with his current host \"<q>yanukovych and russian president vladimir putin have talked by phone but have not met<q>the ousted president expresses surprise that putin has not done more\n"
     ]
    }
   ],
   "source": [
    "for key, value in train_set[0].items():\n",
    "    print(f'key={key}, len={len(value)}, value={value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_iter = DataLoader(train_set, batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_feature \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(train_iter))\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(train_feature)\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:264\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    204\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:127\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mMapping):\n\u001b[0;32m    126\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m         \u001b[39mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem})\n\u001b[0;32m    128\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m         \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[0;32m    130\u001b[0m         \u001b[39mreturn\u001b[39;00m {key: collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem}\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:127\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mMapping):\n\u001b[0;32m    126\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m         \u001b[39mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m batch], collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem})\n\u001b[0;32m    128\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m         \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[0;32m    130\u001b[0m         \u001b[39mreturn\u001b[39;00m {key: collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem}\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:138\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    136\u001b[0m elem_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mnext\u001b[39m(it))\n\u001b[0;32m    137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(elem) \u001b[39m==\u001b[39m elem_size \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m it):\n\u001b[1;32m--> 138\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39meach element in list of batch should be of equal size\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "train_feature = next(iter(train_iter))\n",
    "print(train_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
