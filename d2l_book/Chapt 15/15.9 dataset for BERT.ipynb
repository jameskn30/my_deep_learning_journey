{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNBLmYNXM8hiCQeFTN3pADM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Chapt 15.9, Dataset for pretraining BERT\n","https://d2l.ai/chapter_natural-language-processing-pretraining/bert-dataset.html\n","\n","\n"],"metadata":{"id":"JUXSp-CIQXkf"}},{"cell_type":"code","source":["!pip install setuptools==66\n","!pip install matplotlib_inline\n","!pip install d2l==1.0.0b"],"metadata":{"id":"2IYF2TkeQ3o1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681824085082,"user_tz":240,"elapsed":43426,"user":{"displayName":"James Nguyen (JK)","userId":"02481723335785605125"}},"outputId":"d1b7e2ce-6830-4bdf-b4fb-878caef82b07"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting setuptools==66\n","  Downloading setuptools-66.0.0-py3-none-any.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: setuptools\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 67.6.1\n","    Uninstalling setuptools-67.6.1:\n","      Successfully uninstalled setuptools-67.6.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed setuptools-66.0.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: matplotlib_inline in /usr/local/lib/python3.9/dist-packages (0.1.6)\n","Requirement already satisfied: traitlets in /usr/local/lib/python3.9/dist-packages (from matplotlib_inline) (5.7.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting d2l==1.0.0b\n","  Downloading d2l-1.0.0b0-py3-none-any.whl (141 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.6/141.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from d2l==1.0.0b) (1.22.4)\n","Collecting jupyter\n","  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from d2l==1.0.0b) (2.27.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from d2l==1.0.0b) (1.10.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from d2l==1.0.0b) (1.5.3)\n","Collecting gym==0.21.0\n","  Downloading gym-0.21.0.tar.gz (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from d2l==1.0.0b) (3.7.1)\n","Collecting gpytorch\n","  Downloading gpytorch-1.10-py3-none-any.whl (255 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.9/dist-packages (from d2l==1.0.0b) (0.1.6)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym==0.21.0->d2l==1.0.0b) (2.2.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from gpytorch->d2l==1.0.0b) (1.2.2)\n","Collecting linear-operator>=0.4.0\n","  Downloading linear_operator-0.4.0-py3-none-any.whl (156 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.7/156.7 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: ipykernel in /usr/local/lib/python3.9/dist-packages (from jupyter->d2l==1.0.0b) (5.5.6)\n","Collecting qtconsole\n","  Downloading qtconsole-5.4.2-py3-none-any.whl (121 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.2/121.2 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: ipywidgets in /usr/local/lib/python3.9/dist-packages (from jupyter->d2l==1.0.0b) (7.7.1)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.9/dist-packages (from jupyter->d2l==1.0.0b) (6.5.4)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.9/dist-packages (from jupyter->d2l==1.0.0b) (6.4.8)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.9/dist-packages (from jupyter->d2l==1.0.0b) (6.1.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->d2l==1.0.0b) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->d2l==1.0.0b) (23.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->d2l==1.0.0b) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->d2l==1.0.0b) (2.8.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->d2l==1.0.0b) (1.0.7)\n","Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->d2l==1.0.0b) (5.12.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->d2l==1.0.0b) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->d2l==1.0.0b) (4.39.3)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->d2l==1.0.0b) (8.4.0)\n","Requirement already satisfied: traitlets in /usr/local/lib/python3.9/dist-packages (from matplotlib-inline->d2l==1.0.0b) (5.7.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->d2l==1.0.0b) (2022.7.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->d2l==1.0.0b) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->d2l==1.0.0b) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->d2l==1.0.0b) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->d2l==1.0.0b) (1.26.15)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->d2l==1.0.0b) (3.15.0)\n","Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.9/dist-packages (from linear-operator>=0.4.0->gpytorch->d2l==1.0.0b) (2.0.0+cu118)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->d2l==1.0.0b) (1.16.0)\n","Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter->d2l==1.0.0b) (7.34.0)\n","Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter->d2l==1.0.0b) (6.2)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter->d2l==1.0.0b) (6.1.12)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter->d2l==1.0.0b) (0.2.0)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->jupyter->d2l==1.0.0b) (3.6.4)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->jupyter->d2l==1.0.0b) (3.0.7)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from jupyter-console->jupyter->d2l==1.0.0b) (3.0.38)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from jupyter-console->jupyter->d2l==1.0.0b) (2.14.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0b) (2.1.2)\n","Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0b) (5.3.0)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0b) (4.9.2)\n","Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0b) (3.1.2)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0b) (0.4)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0b) (0.2.2)\n","Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0b) (5.8.0)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0b) (6.0.0)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0b) (1.2.1)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0b) (0.7.3)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0b) (1.5.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0b) (0.7.1)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0b) (0.8.4)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0b) (4.11.2)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter->d2l==1.0.0b) (0.17.1)\n","Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter->d2l==1.0.0b) (1.5.6)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter->d2l==1.0.0b) (21.3.0)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter->d2l==1.0.0b) (23.2.1)\n","Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter->d2l==1.0.0b) (1.8.0)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter->d2l==1.0.0b) (0.16.0)\n","Collecting qtpy>=2.0.1\n","  Downloading QtPy-2.3.1-py3-none-any.whl (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->gpytorch->d2l==1.0.0b) (3.1.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->gpytorch->d2l==1.0.0b) (1.2.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0b) (0.2.0)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0b) (4.8.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0b) (66.0.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0b) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0b) (0.7.5)\n","Collecting jedi>=0.16\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter->d2l==1.0.0b) (3.2.0)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.9/dist-packages (from nbformat>=5.1->nbconvert->jupyter->d2l==1.0.0b) (4.3.3)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.9/dist-packages (from nbformat>=5.1->nbconvert->jupyter->d2l==1.0.0b) (2.16.3)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->d2l==1.0.0b) (0.2.6)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.9/dist-packages (from terminado>=0.8.3->notebook->jupyter->d2l==1.0.0b) (0.7.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch->d2l==1.0.0b) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch->d2l==1.0.0b) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch->d2l==1.0.0b) (3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch->d2l==1.0.0b) (3.11.0)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.11->linear-operator>=0.4.0->gpytorch->d2l==1.0.0b) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.11->linear-operator>=0.4.0->gpytorch->d2l==1.0.0b) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.11->linear-operator>=0.4.0->gpytorch->d2l==1.0.0b) (16.0.1)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.9/dist-packages (from argon2-cffi->notebook->jupyter->d2l==1.0.0b) (21.2.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->nbconvert->jupyter->d2l==1.0.0b) (2.4)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach->nbconvert->jupyter->d2l==1.0.0b) (0.5.1)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0b) (0.8.3)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->d2l==1.0.0b) (0.19.3)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->d2l==1.0.0b) (22.2.0)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->d2l==1.0.0b) (1.15.1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.11->linear-operator>=0.4.0->gpytorch->d2l==1.0.0b) (1.3.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->d2l==1.0.0b) (2.21)\n","Building wheels for collected packages: gym\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for gym (setup.py) ... \u001b[?25lerror\n","\u001b[31m  ERROR: Failed building wheel for gym\u001b[0m\u001b[31m\n","\u001b[0m\u001b[?25h  Running setup.py clean for gym\n","Failed to build gym\n","Installing collected packages: qtpy, jedi, gym, qtconsole, jupyter, linear-operator, gpytorch, d2l\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","  Running setup.py install for gym ... \u001b[?25l\u001b[?25hdone\n","\u001b[33m  DEPRECATION: gym was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. pip 23.1 will enforce this behaviour change. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n","\u001b[0mSuccessfully installed d2l-1.0.0b0 gpytorch-1.10 gym-0.21.0 jedi-0.18.2 jupyter-1.0.0 linear-operator-0.4.0 qtconsole-5.4.2 qtpy-2.3.1\n"]}]},{"cell_type":"code","source":["import collections\n","import math\n","import os\n","import random\n","import torch\n","from d2l import torch as d2l\n","from torch import nn"],"metadata":{"id":"1hG0sxMSQ5iy","executionInfo":{"status":"ok","timestamp":1681824092828,"user_tz":240,"elapsed":7749,"user":{"displayName":"James Nguyen (JK)","userId":"02481723335785605125"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"zbuWfbxSQWP1","executionInfo":{"status":"ok","timestamp":1681824092829,"user_tz":240,"elapsed":5,"user":{"displayName":"James Nguyen (JK)","userId":"02481723335785605125"}}},"outputs":[],"source":["d2l.DATA_HUB['wikitext-2'] = (\n","  'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n","  'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n","\n","def _read_wiki(data_dir, debug = False):\n","  file_name = os.path.join(data_dir, 'wiki.train.tokens')\n","  with open(file_name, 'r') as f:\n","    lines = f.readlines()\n","\n","  # each paragraph is seperated by dot. This is the format of wikitext dataset\n","  # so each element in pargraphs is an array of sentences\n","  paragraphs = [line.strip().lower().split(' . ') \n","  for line in lines if len(line.split(' . ')) >= 2]\n","\n","  #shuffle the paragrams\n","  if debug:\n","    print('total paragraphs ', len(paragraphs))\n","    print('sample paragraph')\n","    for i in range(5):\n","      print(paragraphs[random.randint(1,len(paragraphs))])\n","\n","  random.shuffle(paragraphs)\n","\n","  return paragraphs\n","\n","#definition of _get_tokens_and_segments\n","#we're gonna use \n","def get_tokens_and_segments(tokens_a, tokens_b = None):\n","  tokens = ['<cls>'] + tokens_a + ['<sep>']\n","  segments = [0] * (len(tokens_a) + 2)\n","  # if token_b none: --> (<cls> <token1> ... <sep>) \n","  if tokens_b is not None:\n","    # if token_b not none: --> (<cls> <tokena> ... <sep> <tokenb> ... <sep>) \n","    tokens += tokens_b + ['<sep>']\n","    segments += [1] * (len(tokens_b) + 1)\n","  #segments = [0,0,0,...,1,1,1] if token b is not none\n","  #else = [0,0,0...]\n","  return tokens, segments\n","\n","def _get_next_sentence(sentence, next_sentence, paragraphs):\n","  if random.random() < 0.5:\n","    is_next = True\n","  \n","  else:\n","    next_sentence = random.choice(random.choice(paragraphs))\n","    is_next = False\n","  \n","  return sentence, next_sentence, is_next\n","\n","#prepare data for next sentence prediction\n","def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n","  nsp_data_from_paragraph = []\n","\n","  for i in range(len(paragraph) - 1):\n","    token_a, token_b, is_next = _get_next_sentence(\n","      paragraph[i],\n","      paragraph[i + 1],\n","      paragraphs\n","    )\n","\n","    if len(token_a) + len(token_b) + 3 > max_len: continue\n","  \n","    tokens, segments = d2l.get_tokens_and_segments(token_a, token_b)\n","\n","    nsp_data_from_paragraph.append((tokens, segments, is_next))\n","\n","  return nsp_data_from_paragraph\n","\n","def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab):\n","  #something like [token1, token2, ...]\n","  mlm_input_tokens = [token for token in tokens]\n","  pred_positions_and_labels = []\n","\n","  #shuffle the index of to be predicted token\n","  random.shuffle(candidate_pred_positions)\n","\n","  for mlm_pred_position in candidate_pred_positions:\n","    #if pred positions must not exceed num_mlm_preds \n","    if len(pred_positions_and_labels) >= num_mlm_preds:\n","      break\n","    \n","    masked_token = None\n","\n","    #80% of the time, replace tokens with <mask>\n","    if random.random() > 0.8:\n","      masked_token = '<mask>'\n","    else:\n","      # because this falls in 20% (0.2), \n","      #10% (or 0.5 of 20%) keep the word unchange\n","      if random.random() < 0.5:\n","        masked_token = tokens[mlm_pred_position]\n","      # the remaining 10% (other 0.5 of that that 20%), pick a random word from vocab\n","      else:\n","        masked_token = random.choice(vocab.idx_to_token)\n","      \n","    #change that mlm_pred_position to masked token\n","    mlm_input_tokens[mlm_pred_position] = masked_token\n","    #put the ground truth at that position\n","    pred_positions_and_labels.append((mlm_pred_position, tokens[mlm_pred_position]))\n","\n","  return mlm_input_tokens, pred_positions_and_labels\n","\n","def _get_mlm_data_from_tokens(tokens, vocab):\n","  #index of tokens\n","  candidate_pred_positions = []\n","\n","  for i, token in enumerate(tokens):\n","    #ignore these tokens\n","    if token in ['<cls>', '<sep>']:\n","      continue\n","    candidate_pred_positions.append(i)\n","  \n","  # num mlm preds is set to 15% of all tokens length\n","  num_mlm_preds = max(1, round(len(tokens) * 0.15))\n","\n","  mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab)\n","  pred_positions_and_labels = sorted(pred_positions_and_labels, key = lambda x: x[0])\n","\n","  pred_positions = [v[0] for v in pred_positions_and_labels]\n","  mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n","\n","  return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]"]},{"cell_type":"code","source":["#Padding the BERT input\n","#return: \n","#   all_token_ids,\n","#   all_segments, \n","#   valid_lens, \n","#   all_pred_positions, \n","#   all_mlm_weights, \n","#   all_mlm_labels, \n","#   nsp_labels\n","\n","def _pad_bert_inputs(examples, max_len, vocab):\n","  max_num_mlm_preds = round(len(examples) * 0.15)\n","  all_token_ids, all_segments, valid_lens = [], [], []  \n","  all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n","  nsp_labels = []\n","\n","  for token_ids, pred_positions, mlm_pred_label_ids, segments, is_next in examples:\n","    #pad token ids with vocab['<pad>']\n","    all_token_ids.append(\n","        torch.tensor(token_ids + [vocab['<pad>']] * (max_len - len(token_ids)), dtype=torch.long)\n","    )\n","    #pad segments with 0s\n","    all_segments.append(\n","        torch.tensor(segments + [0] * (max_len - len(segments)), dtype=torch.long)\n","    )\n","    valid_lens.append(torch.tensor(len(token_ids), dtype=torch.long))\n","    #pad pad_predictions with 0s\n","    all_pred_positions.append(\n","      torch.tensor([pred_positions + [0] * (max_num_mlm_preds - len(pred_positions))], dtype=torch.long)\n","    )\n","    # use 1.0 to the length of mlm_pred_label_ids, the rest uses 0s, max len is max_num_mlm_nums\n","    all_mlm_weights.append(\n","      torch.tensor([1.0] * len(mlm_pred_label_ids) + [0] * (max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.float32)\n","    )\n","    #pad mlm_labels with 0s\n","    all_mlm_labels.append(\n","      torch.tensor(mlm_pred_label_ids + [0] * (max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long)\n","    )\n","    #nsp labels \n","    nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n","  \n","  return (all_token_ids, all_segments, valid_lens, all_pred_positions, all_mlm_weights, all_mlm_labels, nsp_labels)\n"],"metadata":{"id":"um7HAglNVLrO","executionInfo":{"status":"ok","timestamp":1681824092829,"user_tz":240,"elapsed":4,"user":{"displayName":"James Nguyen (JK)","userId":"02481723335785605125"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class _WikiTextDataset(torch.utils.data.Dataset):\n","  def __init__(self, paragraphs, max_len, debug = False):\n","    #d2l.tokenize seperates each word in each paragraph --> [word] for each paragraph\n","    #format[[para1], [para2], ...]\n","    paragraphs = [d2l.tokenize(p, token = 'word') for p in paragraphs]\n","    #sentences get all the words in paragraphs\n","    #     ['word1','word2', ...]\n","    sentences = [sentence for p in paragraphs for sentence in p]\n","\n","    self.vocab = d2l.Vocab(sentences, min_freq = 5, reserved_tokens=['<cls>', '<sep>', '<mask>', '<pad>'])\n","\n","    examples = []\n","    if debug:\n","      print()\n","      print('len vocab =', len(self.vocab))\n","      print('getting nsp data from paragraphs')\n","\n","    for index, p in enumerate(paragraphs):\n","      #get pretraining for next sentence prediction\n","      # output of _get_nsp_data_from_paragraph: [(tokens, segments, is_next), ...]\n","      # reminder that tokens = <cls> token a + <sep> + tokenb + <sep>\n","      examples.extend(_get_nsp_data_from_paragraph(p, paragraphs, self.vocab, max_len))\n","  \n","    #get data for masked language modeling\n","    # combine tuples in python (a,b,c) + (d,e) = (a,b,c,d,e)\n","    examples = [(_get_mlm_data_from_tokens(tokens, self.vocab) + (segments, is_next))\\\n","                for tokens, segments, is_next in examples]\n","\n","    if debug:\n","      print('sample of example')\n","      print('masked tokens ', examples[0][0])\n","      print('pred positions ', examples[0][1])\n","      print('mlm pred labels', examples[0][2])\n","      print('segments ', examples[0][3])\n","      print('is_next ', examples[0][4])\n","    \n","    #pad input\n","    (self.all_token_ids, \n","      self.all_segments, \n","      self.valid_lens, \n","      self.all_pred_positions, \n","      self.mlm_weights, \n","      self.all_mlm_labels, \n","      self.nsp_labels) = _pad_bert_inputs(examples, max_len, self.vocab)\n","\n","  def __getitem__(self, i):\n","    #getter\n","    return \n","    (self.all_token_ids[i], \n","      self.all_segments[i], \n","      self.valid_lens[i], \n","      self.all_pred_positions[i], \n","      self.mlm_weights[i], \n","      self.all_mlm_labels[i], \n","      self.nsp_labels[i])\n","  \n","  def __len__(self):\n","    return len(self.all_token_ids)\n","\n","#Just a testing some functionality\n","p = [\"in 1881 the observatory 's director , charles <unk> , suggested adding a high @-@ quality telescope to the observatory\",\n","     'he felt that direct solar observations would lead to a better understanding of sunspot effects on weather ( as late as 1910 the observatory \\'s then @-@ director , r. f. <unk> , noted that \" sun spots have more to do with our weather conditions than have the rings around the moon',\n","     '\" )', \n","     '<unk> , the canadian government ( having formed in 1867 ) was interested in taking part in the major international effort to accurately record the december 1882 transit of venus .']\n","    \n","sample1 = d2l.tokenize(p, token='word')\n","print(sample1)\n","\n","print([s for s in sample1[0]])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WQ2BRUodXJBA","executionInfo":{"status":"ok","timestamp":1681824092830,"user_tz":240,"elapsed":5,"user":{"displayName":"James Nguyen (JK)","userId":"02481723335785605125"}},"outputId":"0cf943c9-b902-4215-a3a4-b320721aa52b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[['in', '1881', 'the', 'observatory', \"'s\", 'director', ',', 'charles', '<unk>', ',', 'suggested', 'adding', 'a', 'high', '@-@', 'quality', 'telescope', 'to', 'the', 'observatory'], ['he', 'felt', 'that', 'direct', 'solar', 'observations', 'would', 'lead', 'to', 'a', 'better', 'understanding', 'of', 'sunspot', 'effects', 'on', 'weather', '(', 'as', 'late', 'as', '1910', 'the', 'observatory', \"'s\", 'then', '@-@', 'director', ',', 'r.', 'f.', '<unk>', ',', 'noted', 'that', '\"', 'sun', 'spots', 'have', 'more', 'to', 'do', 'with', 'our', 'weather', 'conditions', 'than', 'have', 'the', 'rings', 'around', 'the', 'moon'], ['\"', ')'], ['<unk>', ',', 'the', 'canadian', 'government', '(', 'having', 'formed', 'in', '1867', ')', 'was', 'interested', 'in', 'taking', 'part', 'in', 'the', 'major', 'international', 'effort', 'to', 'accurately', 'record', 'the', 'december', '1882', 'transit', 'of', 'venus', '.']]\n","['in', '1881', 'the', 'observatory', \"'s\", 'director', ',', 'charles', '<unk>', ',', 'suggested', 'adding', 'a', 'high', '@-@', 'quality', 'telescope', 'to', 'the', 'observatory']\n"]}]},{"cell_type":"code","source":["#@title Load pretraining data for BERT\n","batch_size = 512 #@param {type:\"number\"}\n","max_len = 64 #@param {type:\"number\"}\n","debug = True #@param {type:\"boolean\"}\n","\n","#Download wikitext 2 and use WikiTextDataset object to generate pretraining example\n","def load_data_wiki(batch_size, max_len, debug = False):\n","  num_workers = d2l.get_dataloader_workers()\n","  data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n","\n","  #get paragraphs\n","  paragraphs = _read_wiki(data_dir,debug) \n","  train_set = _WikiTextDataset(paragraphs, max_len)\n","\n","  train_iter = torch.utils.data.DataLoader(\n","      train_set, batch_size, \n","      shuffle = True, num_workers=num_workers)\n","\n","  return train_set, train_iter, train_set.vocab\n","\n","train_set, train_iter, vocab = load_data_wiki(batch_size, max_len, debug)\n","print(train_set[0])\n","print(len(vocab))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BnRLnVIFaomT","outputId":"7ad7b47a-35e0-43c9-b568-7d1b851b3f31"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total paragraphs  15496\n","sample paragraph\n","['wang \\'s tale portrays zhou as an aging itinerant <unk> with \" a fame <unk> like thunder \" throughout the underworld society of <unk>', 'he is made the sworn brother of the outlaw \" <unk> <unk> \" lu zhishen , a military officer @-@ turned @-@ fighting monk , who is , according to hsia , first among the most popular protagonists of the water margin', 'he is also given the nickname \" iron arm \" ( <unk> ) , which carried over into the title of his fictional biography iron arm , golden sabre', 'while the tale fails to explain the reason for the moniker , it does mention zhou \\'s ability to direct his <unk> to any part of his body to make it hard enough to <unk> the \" iron shirt \" technique of another martial artist', 'furthermore , zhou shares the same nickname with cai fu , an executioner @-@ turned @-@ outlaw known for his ease in wielding a heavy sword .']\n","[\"silver bullet 's layout passes through three of the park 's themed areas : ghost town , <unk> village , and indian trails .\"]\n","['dershowitz threatened libel action over the charges in finkelstein \\'s book , as a consequence of which , the publisher deleted the word \" plagiarism \" from the text before publication', 'finkelstein agreed to remove the suggestion that dershowitz was not the true author of the case for israel because , as the publisher said , \" he couldn \\'t document that \" .']\n","['around the same time , the jtwc upgraded the system to a tropical storm , estimating the cyclone to have attained peak winds of 65 km / h ( 40 mph 1 @-@ minute sustained )', 'on 19 may , satellite imagery of the system depicted that a new low pressure centre had developed roughly 300 km ( 190 mi ) south of the original low', \"several hours after the relocation , the jtwc downgraded herbie to a tropical depression as the system 's movement began to accelerate towards the southeast\", 'as the storm moved at a rapid speed towards the coastline of western australia , it began to undergo an extratropical transition', 'during a 24 @-@ hour period ( 20 – 21 may ) herbie tracked roughly 1 @,@ 500 km ( 930 mi ) , with the movement of the storm reaching 70 km / h ( 43 mph ) at times .']\n","['during the christmas season , the city of sarnia presents the annual \" celebration of lights \" in centennial park', \"the event was created in 1984 by dr. wills <unk> and a committee funded by the retail chain hudson 's bay , and the national telecommunications company <unk>\", \"from modest beginnings the event has garnered numerous awards as it has grown , including second place in the 2002 canadian government 's canada <unk> competition\", 'the celebration , was incorporated in its national <unk> year and is now run by a voluntary board of directors .']\n"]}]},{"cell_type":"code","source":["print(next(iter(train_iter)))\n"],"metadata":{"id":"Amo-UluleKa0","colab":{"base_uri":"https://localhost:8080/","height":484},"executionInfo":{"status":"error","timestamp":1681824192999,"user_tz":240,"elapsed":293,"user":{"displayName":"James Nguyen (JK)","userId":"02481723335785605125"}},"outputId":"f5bd7ef5-6939-4a86-aae1-2426a0988e17"},"execution_count":7,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-4e3200085ea4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1346\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/collate.py\", line 264, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/collate.py\", line 150, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"pyvXpNWt1HYF","executionInfo":{"status":"aborted","timestamp":1681824193000,"user_tz":240,"elapsed":10,"user":{"displayName":"James Nguyen (JK)","userId":"02481723335785605125"}}},"execution_count":null,"outputs":[]}]}