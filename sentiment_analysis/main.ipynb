{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[device(type='cuda', index=0)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import logging \n",
    "from d2l import torch as d2l\n",
    "\n",
    "devices = d2l.try_all_gpus()\n",
    "\n",
    "print(devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "#src: https://classic.d2l.ai/_modules/d2l/mxnet.html#load_data_imdb\n",
    "\n",
    "d2l.DATA_HUB['aclImdb'] = (\n",
    "    'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',\n",
    "    '01ada507287d82875905620988597833ad4e0903')\n",
    "\n",
    "def read_imdb(data_dir, is_train):\n",
    "    \"\"\"Read the IMDb review dataset text sequences and labels.\n",
    "\n",
    "    Defined in :numref:`sec_sentiment`\"\"\"\n",
    "    data, labels = [], []\n",
    "    for label in ('pos', 'neg'):\n",
    "        folder_name = os.path.join(data_dir, 'train' if is_train else 'test',\n",
    "                                   label)\n",
    "        for file in os.listdir(folder_name):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', '')\n",
    "                data.append(review)\n",
    "                labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "def load_data_imdb(batch_size, num_steps=500):\n",
    "    \"\"\"Return data iterators and the vocabulary of the IMDb review dataset.\n",
    "    Defined in :numref:`sec_sentiment`\"\"\"\n",
    "    data_dir = d2l.download_extract('aclImdb', 'aclImdb')\n",
    "    train_data = read_imdb(data_dir, True)\n",
    "    test_data = read_imdb(data_dir, False)\n",
    "    train_tokens = d2l.tokenize(train_data[0], token='word')\n",
    "    test_tokens = d2l.tokenize(test_data[0], token='word')\n",
    "    vocab = d2l.Vocab(train_tokens, min_freq=5)\n",
    "    train_features = np.array([d2l.truncate_pad(\n",
    "        vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])\n",
    "    test_features = np.array([d2l.truncate_pad(\n",
    "        vocab[line], num_steps, vocab['<pad>']) for line in test_tokens])\n",
    "    train_iter = d2l.load_array((train_features, train_data[1]), batch_size)\n",
    "    test_iter = d2l.load_array((test_features, test_data[1]), batch_size,\n",
    "                               is_train=False)\n",
    "    return train_iter, test_iter, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, **kwargs):\n",
    "    super(BiRNN, self).__init__(**kwargs)\n",
    "\n",
    "    # self.embedding = BertModel.from_pretrained('bert-base-uncased')\n",
    "    self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "    self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers = num_layers, bidrectional=True)\n",
    "\n",
    "    self.decoder = nn.Linear(4 * num_hiddens, 2)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    # original inputs = (# batch, # steps)\n",
    "    #transform inputs to (# steps, # batch)\n",
    "    embeddings = self.embedding(inputs.T)\n",
    "    self.encoder.flatten_parameters()\n",
    "    #output shape = (# steps, # batch, # word vector dim)\n",
    "    outputs, _ = self.encoder(embeddings)\n",
    "\n",
    "    print('output shape = ', outputs.shape)\n",
    "\n",
    "    encoding = torch.cat((outputs[0], outputs[-1]), dim = 1)\n",
    "    print('encoding shape after concat = ',encoding.shape)\n",
    "\n",
    "    outs = self.decoder(encoding) \n",
    "\n",
    "    return outs\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers, devices = 100, 100, 2, d2l.try_all_gpus()\n",
    "net = BiRNN(len(vocab), num_hiddens, num_layers,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
